Title: CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models

URL Source: https://arxiv.org/html/2410.21067v1

Markdown Content:
Meiqi Chen1111Work was done during an internship at Pattern Recognition Center, WeChat AI, Tencent Inc., Fandong Meng2222Corresponding author., Yingxue Zhang2, Yan Zhang1, Jie Zhou2  
1Peking University 2Pattern Recognition Center, WeChat AI, Tencent Inc, China  
meiqichen@stu.pku.edu.cn, zhyzhy001@pku.edu.cn,  
{fandongmeng, yxuezhang, withtomzhou}@tencent.com

###### Abstract

Large language models (LLMs) have shown great promise in machine translation, but they still struggle with contextually dependent terms, such as new or domain-specific words. This leads to inconsistencies and errors that are difficult to address. Existing solutions often depend on manual identification of such terms, which is impractical given the complexity and evolving nature of language. While Retrieval-Augmented Generation (RAG) could provide some assistance, its application to translation is limited by issues such as hallucinations from information overload. In this paper, we propose CRAT, a novel multi-agent translation framework that leverages RAG and causality-enhanced self-reflection to address these challenges. This framework consists of several specialized agents: the Unknown Terms Identification agent detects unknown terms within the context, the Knowledge Graph (KG) Constructor agent extracts relevant internal knowledge about these terms and retrieves bilingual information from external sources, the Causality-enhanced Judge agent validates the accuracy of the information, and the Translator agent incorporates the refined information into the final output. This automated process allows for more precise and consistent handling of key terms during translation. Our results show that CRAT significantly improves translation accuracy, particularly in handling context-sensitive terms and emerging vocabulary.

CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models

Meiqi Chen1111Work was done during an internship at Pattern Recognition Center, WeChat AI, Tencent Inc., Fandong Meng2222Corresponding author., Yingxue Zhang2, Yan Zhang1, Jie Zhou2 1Peking University 2Pattern Recognition Center, WeChat AI, Tencent Inc, China meiqichen@stu.pku.edu.cn, zhyzhy001@pku.edu.cn, {fandongmeng, yxuezhang, withtomzhou}@tencent.com

## 1 Introduction

Large language models (LLMs), such as GPT-4 OpenAI ([2023](https://arxiv.org/html/2410.21067v1#bib.bib15)) and Llama Touvron et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib23)), are increasingly becoming foundational for NLP tasks. Recently, several efforts have been made to leverage the capabilities of these models for machine translation Hendy et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib12)); Jiao et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib13)); Zhang et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib30)); Wang et al. ([2024b](https://arxiv.org/html/2410.21067v1#bib.bib26)), achieving promising results that demonstrate their potential in practical applications.

![Image 1: Refer to caption](https://arxiv.org/html/2410.21067v1/x1.png)

Figure 1: {CJK\*}UTF8gbsn Demonstration of the contextual ambiguity LLMs face during English-Chinese translation. In the top example, “_bank_” and “_Scotia_” are interpreted within a financial context, while in the bottom example, they shift to a geographical meaning. This illustrates the necessity for combining both internal knowledge and external, domain-specific information to accurately reason about context-dependent terms.

However, using LLMs for translation heavily relies on the model’s generative abilities. This is because LLMs passively learn to model the relationships between contexts based on pretraining corpora Cao et al. ([2022](https://arxiv.org/html/2410.21067v1#bib.bib2)); Sun et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib22)). When a word no longer carries its usual meaning in a particular context, LLMs tend to struggle, and it becomes challenging to maintain consistency in translating entities and referential terms. As shown in Figure [1](https://arxiv.org/html/2410.21067v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models"), where the terms “_bank_” and “_Scotia_” have different meanings depending on the context. In the first case, these terms refer to a financial institution and its services, while in the second, they shift to a geographical and navigational context. This contextual ambiguity demonstrates the need for LLMs to access both internal knowledge (from the original context) and external knowledge (updated or domain-specific information) to accurately infer and translate terms based on the situation.

This challenge underscores the importance of adapting LLMs to practical translation scenarios. The key to solving this problem lies in recognizing special terms that need attention and determining their precise meanings. Previous methods typically rely on researchers’ prior knowledge to manually identify these terms, including acronyms, new words, slang, or borrowed words Fiederer and O’Brien ([2009](https://arxiv.org/html/2410.21067v1#bib.bib8)); Choi et al. ([2017](https://arxiv.org/html/2410.21067v1#bib.bib5)); Vaibhav et al. ([2019](https://arxiv.org/html/2410.21067v1#bib.bib24)). However, given the complexity and diversity of real-world data, manually identifying terms one by one is impractical. Furthermore, the dynamic nature of language leads to the constant emergence of new word meanings, making it challenging for LLMs to translate accurately, especially when the models have not been updated.

Thus, there is a pressing need for an automated approach to identify and clarify the meanings of such terms. While retrieval-augmented generation (RAG) has been applied to question-answering (QA) tasks Ye et al. ([2021](https://arxiv.org/html/2410.21067v1#bib.bib28)); Asai et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib1)), it poses challenges when used directly for translation. Excessive external information can interfere with accurate translation, leading to errors or hallucinations Gao et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib9)); Ding et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib7)).

To address this issue, in this paper, we propose CRAT, a multi-agent framework that leverages the powerful instruction-following and tool-using capabilities of LLMs for Causality-Enhanced Reflective and Retrieval-Augmented Translation. This framework allows LLMs to autonomously identify unknown terms and clarify their meanings within the given context. Specifically, an Unknown Terms Detector agent identifies unknown terms in the context. Next, a Knowledge Graph (KG) Constructor agent extracts internal knowledge about these terms from the context and retrieves relevant information from external sources, to construct a KG for translation (a.k.a., TransKG). Then, a Causality-enhanced Judge agent uses causality-enhanced reflection to evaluate whether the retrieved information is relevant and appropriate, facilitating the construction of TransKG. Finally, a Translation agent references the updated information to produce a precise and consistent translation.

Overall, our main contributions are as follows:

- •We introduce a novel framework for retrieval-augmented machine translation, which could automatically gather internal and external information on unknown terms to construct a TransKG, reducing the reliance on manual intervention.
- •We make the first attempt to apply a causality-enhanced reflection mechanism for retrieval-augmented machine translation, which ensures contextual meanings and improves translation accuracy.
- •Experimental results demonstrate that our framework significantly improves translation accuracy and consistency.

## 2 Related Work

### 2.1 LLMs for Machine Translation

Recent work on LLMs for machine translation highlights both their potential and their limitations, particularly when dealing with context-dependent or domain-specific terms Peng et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib19)); Hendy et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib12)); Jiao et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib13)); Zhang et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib30)). For example, He et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib11)) explores human-like translation strategies by encouraging LLMs to imitate human reasoning, which improves their ability to handle nuanced contexts. Similarly, Zeng et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib29)) and Wang et al. ([2024a](https://arxiv.org/html/2410.21067v1#bib.bib25)) introduce self-reflection mechanisms that enable LLMs to assess and refine their own translations, enhancing overall accuracy. However, these approaches still rely heavily on the model’s pretraining and may struggle with newly emerging terms or ambiguous contexts. Ghazvininejad et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib10)) investigate dictionary-based prompting to improve phrase-level translation, demonstrating the value of incorporating external knowledge into the translation process. While effective, this method requires structured input, which can limit its flexibility in more dynamic scenarios. Our work overcomes this by integrating both internal and external knowledge sources dynamically through a multi-agent framework, automating the identification and clarification of unknown terms.

### 2.2 Retrieval Augmented Translation

RAG has been widely applied in tasks such as question answering Ye et al. ([2021](https://arxiv.org/html/2410.21067v1#bib.bib28)); Asai et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib1)), but its use in translation remains relatively limited. One major challenge is hallucination, where excessive retrieval of information introduces inaccuracies. Zhang et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib31)) applied RAG to the translation of e-commerce product titles, successfully improving accuracy by retrieving domain-specific information. Conia et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib6)) introduce a RAG framework for multilingual machine translation, specifically targeting domain-specific translations. However, these approaches still risk overloading the translation with irrelevant data. Similar challenges are noted in Gao et al. ([2023](https://arxiv.org/html/2410.21067v1#bib.bib9)); Yan et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib27)); Ding et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib7)); Chen et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib3)), where feeding retrieval information directly to LLMs without verification often results in hallucinations.

### 2.3 Causality-aware Generation

Some studies have pointed out that LLMs may produce inaccurate output due to reliance on spurious correlations learned from pretraining corpus Cao et al. ([2022](https://arxiv.org/html/2410.21067v1#bib.bib2)); Sun et al. ([2024](https://arxiv.org/html/2410.21067v1#bib.bib22)). As for the translation task, Chesterman ([2017](https://arxiv.org/html/2410.21067v1#bib.bib4)) propose a causal model where human translations are explicitly seen both as caused by antecedent conditions and as causing effects on readers and cultures. Ni et al. ([2022](https://arxiv.org/html/2410.21067v1#bib.bib14)) studies how “translationese” (text translated by humans) affects machine translation performance through causal analysis. However, to the best of our knowledge, we are the first work to apply causal reasoning mechanisms to LLMs for machine translation tasks. By leveraging causal invariancePearl ([2009](https://arxiv.org/html/2410.21067v1#bib.bib17)); Pearl and Mackenzie ([2018](https://arxiv.org/html/2410.21067v1#bib.bib18)), our approach validates the retrieved information and promotes more accurate translations, particularly for terms and concepts that are ambiguous or not often encountered during pretraining.

Require :

U𝑈Uitalic_U(Unkown Terms Identification),

K𝐾Kitalic_K(Knowledge Graph Constructor),

C𝐶Citalic_C(Causality-enhanced Judge),

T𝑇Titalic_T(Translator)

Input :

x𝑥xitalic_x(Input context)

Output :

y𝑦yitalic_y(Generated translation)

1

U𝑈Uitalic_Uidentifies

M\={m1,…,mN⁢1}𝑀subscript𝑚1…subscript𝑚𝑁1M=\\left\\{m\_{1},\\ldots,m\_{N1}\\right\\}italic_M = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_m start_POSTSUBSCRIPT italic_N 1 end_POSTSUBSCRIPT }

2

K𝐾Kitalic_Kextracts Internal_Knolwedge

kIsubscript𝑘𝐼k\_{I}italic_k start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPTrelated to

M𝑀Mitalic_Mfrom

x𝑥xitalic_x

3

K𝐾Kitalic_Kretrieves documents

D\={d1,d2,…,dN⁢2}𝐷subscript𝑑1subscript𝑑2…subscript𝑑𝑁2D=\\{d\_{1},d\_{2},...,d\_{N2}\\}italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_N 2 end_POSTSUBSCRIPT }from external sources

4 Confidence = Give a final judgment based on the relevance of each pair (

kIsubscript𝑘𝐼k\_{I}italic_k start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT,

disubscript𝑑𝑖d\_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT),

di∈Dsubscript𝑑𝑖𝐷d\_{i}\\in Ditalic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_D

// Confidence has 2 optional values: \[CORRECT\] or \[INCORRECT\]

5 if _ConfidenceisubscriptConfidence𝑖\\mathrm{Confidence}\_{i}roman_Confidence start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT == \[CORRECT\]_ then

6 External_Knowledge

kEsubscript𝑘𝐸k\_{E}italic_k start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT+=

disubscript𝑑𝑖d\_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT

T𝑇Titalic_Tpredicts

y𝑦yitalic_ygiven

x𝑥xitalic_x,

kIsubscript𝑘𝐼k\_{I}italic_k start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT, and

kEsubscript𝑘𝐸k\_{E}italic_k start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT

Algorithm 1 CRAT Inference

## 3 Method

In this section, we outline the proposed multi-agent translation framework (i.e., CRAT) that leverages the Retrieval-Augmented Generation (RAG) mechanism and causality-enhanced self-reflection. The framework addresses the challenge of translating uncertain terms with accuracy and contextual relevance, overcoming limitations observed in LLMs when encountering such terms. The overall pipeline is presented in Figure [2](https://arxiv.org/html/2410.21067v1#S3.F2 "Figure 2 ‣ 3.1 Task Formulation ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models").

### 3.1 Task Formulation

As shown in Algorithm [1](https://arxiv.org/html/2410.21067v1#algorithm1 "In 2.3 Causality-aware Generation ‣ 2 Related Work ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models"), given input source context 𝒳𝒳\\mathcal{X}caligraphic_X, the framework is expected to generate the translation 𝒴𝒴\\mathcal{Y}caligraphic_Y. The entire framework typically includes an unknown terms detector 𝒰𝒰\\mathcal{U}caligraphic_U, a knowledge graph constructor 𝒦𝒦\\mathcal{K}caligraphic_K, a causality-enhanced judge 𝒞𝒞\\mathcal{C}caligraphic_C, and a translator 𝒯𝒯\\mathcal{T}caligraphic_T. The detector 𝒰𝒰\\mathcal{U}caligraphic_U identifies unknown terms ℳ\={m1,…,mN⁢1}ℳsubscript𝑚1…subscript𝑚𝑁1\\mathcal{M}=\\left\\{m\_{1},\\ldots,m\_{N1}\\right\\}caligraphic_M = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_m start_POSTSUBSCRIPT italic_N 1 end_POSTSUBSCRIPT } from the input 𝒳𝒳\\mathcal{X}caligraphic_X. The constructor 𝒦𝒦\\mathcal{K}caligraphic_K extracts internal knowledge 𝒦Isubscript𝒦𝐼\\mathcal{K}\_{I}caligraphic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT related to the unknown terms ℳℳ\\mathcal{M}caligraphic_M from the input 𝒳𝒳\\mathcal{X}caligraphic_X and retrieves relevant documents 𝒟\={d1,…,dN⁢2}𝒟subscript𝑑1…subscript𝑑𝑁2\\mathcal{D}=\\left\\{d\_{1},\\ldots,d\_{N2}\\right\\}caligraphic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_N 2 end_POSTSUBSCRIPT } from external sources. The judge 𝒞𝒞\\mathcal{C}caligraphic_C then evaluates the relevance of each document and determines the refined external knowledge 𝒦Esubscript𝒦𝐸\\mathcal{K}\_{E}caligraphic_K start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT. Based on the input 𝒳𝒳\\mathcal{X}caligraphic_X and the knowledge 𝒦I+𝒦Esubscript𝒦𝐼subscript𝒦𝐸\\mathcal{K}\_{I}+\\mathcal{K}\_{E}caligraphic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT + caligraphic_K start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT, the translator 𝒯𝒯\\mathcal{T}caligraphic_T finally generate the translation 𝒴𝒴\\mathcal{Y}caligraphic_Y. This framework can be formulated as:

<table id="S3.E1"><tbody><tr><td></td><td><math alttext="P(\mathcal{Y}\mid\mathcal{X})=P(\mathcal{K}_{I}\mid\mathcal{X})P(\mathcal{K}_{%
E}\mid\mathcal{X})P(\mathcal{Y}\mid\mathcal{X},\mathcal{K}_{I},\mathcal{K}_{E})" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><mi id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml">P</mi><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">𝒴</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml">∣</mo><mi id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">𝒳</mi></mrow><mo id="S3.E1.m1.2.2.1.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.5" xref="S3.E1.m1.5.5.5.cmml">=</mo><mrow id="S3.E1.m1.5.5.4" xref="S3.E1.m1.5.5.4.cmml"><mi id="S3.E1.m1.5.5.4.5" xref="S3.E1.m1.5.5.4.5.cmml">P</mi><mo id="S3.E1.m1.5.5.4.4" xref="S3.E1.m1.5.5.4.4.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.2.1.1" xref="S3.E1.m1.3.3.2.1.1.1.cmml"><mo id="S3.E1.m1.3.3.2.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.2.1.1.1" xref="S3.E1.m1.3.3.2.1.1.1.cmml"><msub id="S3.E1.m1.3.3.2.1.1.1.2" xref="S3.E1.m1.3.3.2.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.2.1.1.1.2.2" xref="S3.E1.m1.3.3.2.1.1.1.2.2.cmml">𝒦</mi><mi id="S3.E1.m1.3.3.2.1.1.1.2.3" xref="S3.E1.m1.3.3.2.1.1.1.2.3.cmml">I</mi></msub><mo id="S3.E1.m1.3.3.2.1.1.1.1" xref="S3.E1.m1.3.3.2.1.1.1.1.cmml">∣</mo><mi id="S3.E1.m1.3.3.2.1.1.1.3" xref="S3.E1.m1.3.3.2.1.1.1.3.cmml">𝒳</mi></mrow><mo id="S3.E1.m1.3.3.2.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.2.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.4.4a" xref="S3.E1.m1.5.5.4.4.cmml">⁢</mo><mi id="S3.E1.m1.5.5.4.6" xref="S3.E1.m1.5.5.4.6.cmml">P</mi><mo id="S3.E1.m1.5.5.4.4b" xref="S3.E1.m1.5.5.4.4.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.3.2.1" xref="S3.E1.m1.4.4.3.2.1.1.cmml"><mo id="S3.E1.m1.4.4.3.2.1.2" stretchy="false" xref="S3.E1.m1.4.4.3.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.3.2.1.1" xref="S3.E1.m1.4.4.3.2.1.1.cmml"><msub id="S3.E1.m1.4.4.3.2.1.1.2" xref="S3.E1.m1.4.4.3.2.1.1.2.cmml"><mi id="S3.E1.m1.4.4.3.2.1.1.2.2" xref="S3.E1.m1.4.4.3.2.1.1.2.2.cmml">𝒦</mi><mi id="S3.E1.m1.4.4.3.2.1.1.2.3" xref="S3.E1.m1.4.4.3.2.1.1.2.3.cmml">E</mi></msub><mo id="S3.E1.m1.4.4.3.2.1.1.1" xref="S3.E1.m1.4.4.3.2.1.1.1.cmml">∣</mo><mi id="S3.E1.m1.4.4.3.2.1.1.3" xref="S3.E1.m1.4.4.3.2.1.1.3.cmml">𝒳</mi></mrow><mo id="S3.E1.m1.4.4.3.2.1.3" stretchy="false" xref="S3.E1.m1.4.4.3.2.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.4.4c" xref="S3.E1.m1.5.5.4.4.cmml">⁢</mo><mi id="S3.E1.m1.5.5.4.7" xref="S3.E1.m1.5.5.4.7.cmml">P</mi><mo id="S3.E1.m1.5.5.4.4d" xref="S3.E1.m1.5.5.4.4.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.4.3.1" xref="S3.E1.m1.5.5.4.3.1.1.cmml"><mo id="S3.E1.m1.5.5.4.3.1.2" stretchy="false" xref="S3.E1.m1.5.5.4.3.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.4.3.1.1" xref="S3.E1.m1.5.5.4.3.1.1.cmml"><mi id="S3.E1.m1.5.5.4.3.1.1.4" xref="S3.E1.m1.5.5.4.3.1.1.4.cmml">𝒴</mi><mo id="S3.E1.m1.5.5.4.3.1.1.3" xref="S3.E1.m1.5.5.4.3.1.1.3.cmml">∣</mo><mrow id="S3.E1.m1.5.5.4.3.1.1.2.2" xref="S3.E1.m1.5.5.4.3.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝒳</mi><mo id="S3.E1.m1.5.5.4.3.1.1.2.2.3" xref="S3.E1.m1.5.5.4.3.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.4.3.1.1.1.1.1" xref="S3.E1.m1.5.5.4.3.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.4.3.1.1.1.1.1.2" xref="S3.E1.m1.5.5.4.3.1.1.1.1.1.2.cmml">𝒦</mi><mi id="S3.E1.m1.5.5.4.3.1.1.1.1.1.3" xref="S3.E1.m1.5.5.4.3.1.1.1.1.1.3.cmml">I</mi></msub><mo id="S3.E1.m1.5.5.4.3.1.1.2.2.4" xref="S3.E1.m1.5.5.4.3.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.4.3.1.1.2.2.2" xref="S3.E1.m1.5.5.4.3.1.1.2.2.2.cmml"><mi id="S3.E1.m1.5.5.4.3.1.1.2.2.2.2" xref="S3.E1.m1.5.5.4.3.1.1.2.2.2.2.cmml">𝒦</mi><mi id="S3.E1.m1.5.5.4.3.1.1.2.2.2.3" xref="S3.E1.m1.5.5.4.3.1.1.2.2.2.3.cmml">E</mi></msub></mrow></mrow><mo id="S3.E1.m1.5.5.4.3.1.3" stretchy="false" xref="S3.E1.m1.5.5.4.3.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><eq id="S3.E1.m1.5.5.5.cmml" xref="S3.E1.m1.5.5.5"></eq><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><times id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></times><ci id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3">𝑃</ci><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2">𝒴</ci><ci id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">𝒳</ci></apply></apply><apply id="S3.E1.m1.5.5.4.cmml" xref="S3.E1.m1.5.5.4"><times id="S3.E1.m1.5.5.4.4.cmml" xref="S3.E1.m1.5.5.4.4"></times><ci id="S3.E1.m1.5.5.4.5.cmml" xref="S3.E1.m1.5.5.4.5">𝑃</ci><apply id="S3.E1.m1.3.3.2.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.3.3.2.1.1.1.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.2.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.2.2">𝒦</ci><ci id="S3.E1.m1.3.3.2.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.2.1.1.1.2.3">𝐼</ci></apply><ci id="S3.E1.m1.3.3.2.1.1.1.3.cmml" xref="S3.E1.m1.3.3.2.1.1.1.3">𝒳</ci></apply><ci id="S3.E1.m1.5.5.4.6.cmml" xref="S3.E1.m1.5.5.4.6">𝑃</ci><apply id="S3.E1.m1.4.4.3.2.1.1.cmml" xref="S3.E1.m1.4.4.3.2.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.3.2.1.1.1.cmml" xref="S3.E1.m1.4.4.3.2.1.1.1">conditional</csymbol><apply id="S3.E1.m1.4.4.3.2.1.1.2.cmml" xref="S3.E1.m1.4.4.3.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.3.2.1.1.2.1.cmml" xref="S3.E1.m1.4.4.3.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.4.4.3.2.1.1.2.2.cmml" xref="S3.E1.m1.4.4.3.2.1.1.2.2">𝒦</ci><ci id="S3.E1.m1.4.4.3.2.1.1.2.3.cmml" xref="S3.E1.m1.4.4.3.2.1.1.2.3">𝐸</ci></apply><ci id="S3.E1.m1.4.4.3.2.1.1.3.cmml" xref="S3.E1.m1.4.4.3.2.1.1.3">𝒳</ci></apply><ci id="S3.E1.m1.5.5.4.7.cmml" xref="S3.E1.m1.5.5.4.7">𝑃</ci><apply id="S3.E1.m1.5.5.4.3.1.1.cmml" xref="S3.E1.m1.5.5.4.3.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.4.3.1.1.3.cmml" xref="S3.E1.m1.5.5.4.3.1.1.3">conditional</csymbol><ci id="S3.E1.m1.5.5.4.3.1.1.4.cmml" xref="S3.E1.m1.5.5.4.3.1.1.4">𝒴</ci><list id="S3.E1.m1.5.5.4.3.1.1.2.3.cmml" xref="S3.E1.m1.5.5.4.3.1.1.2.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝒳</ci><apply id="S3.E1.m1.5.5.4.3.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.4.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.4.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.4.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.4.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.4.3.1.1.1.1.1.2">𝒦</ci><ci id="S3.E1.m1.5.5.4.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.4.3.1.1.1.1.1.3">𝐼</ci></apply><apply id="S3.E1.m1.5.5.4.3.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.4.3.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.4.3.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.4.3.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.4.3.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.4.3.1.1.2.2.2.2">𝒦</ci><ci id="S3.E1.m1.5.5.4.3.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.4.3.1.1.2.2.2.3">𝐸</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">P(\mathcal{Y}\mid\mathcal{X})=P(\mathcal{K}_{I}\mid\mathcal{X})P(\mathcal{K}_{% E}\mid\mathcal{X})P(\mathcal{Y}\mid\mathcal{X},\mathcal{K}_{I},\mathcal{K}_{E})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_P ( caligraphic_Y ∣ caligraphic_X ) = italic_P ( caligraphic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ∣ caligraphic_X ) italic_P ( caligraphic_K start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ∣ caligraphic_X ) italic_P ( caligraphic_Y ∣ caligraphic_X , caligraphic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , caligraphic_K start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(1)</span></td></tr></tbody></table>

![Image 2: Refer to caption](https://arxiv.org/html/2410.21067v1/x2.png)

Figure 2: Our proposed multi-agent translation framework (i.e., CRAT) for improving LLM translation accuracy, especially for unknown terms. The Unknown Terms Detector Agent identifies terms with low confidence or ambiguity, such as polysemes or new terms. The Knowledge Graph Constructor Agent then builds a TransKG based on both internal and external sources to collect the necessary information. The Causality-enhanced Judge Agent evaluates the appropriateness of term meanings by considering causal invariance in context. Finally, the Translator Agent uses accurate meanings to generate translations.

### 3.2 Unknown Terms Detector

The Unknown Terms Detector agent is responsible for identifying terms within the source context that may pose challenges for accurate translation. These terms typically include words or phrases with low confidence by agent, polysemes (words with multiple meanings, e.g., “_bank_” in Figure [2](https://arxiv.org/html/2410.21067v1#S3.F2 "Figure 2 ‣ 3.1 Task Formulation ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models")), acronyms (e.g., F.B.I for Federal Bureau of Investigation), proper nouns (e.g., “_Sccotia_” in Figure [2](https://arxiv.org/html/2410.21067v1#S3.F2 "Figure 2 ‣ 3.1 Task Formulation ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models")), or new terms that the agent may not fully comprehend. By autonomously identifying these terms, the agent highlights them for further analysis, ensuring that they receive special attention during the translation process. This step is crucial for maintaining translation accuracy, as it allows the framework to proactively address potential ambiguities or inconsistencies before the translation is finalized.

### 3.3 Knowledge Graph Constructor

The Knowledge Graph Constructor plays a crucial role in enhancing translation accuracy by collecting internal and external knowledge related to the detected unknown terms, thereby resolving contextual ambiguities. Specifically, it constructs a Translation Knowledge Graph (TransKG), a structured representation of term relationships and meanings based on internal and external knowledge.

#### Internal Knowledge Extraction

The agent begins by identifying and structuring internal knowledge directly from the source context. It identifies relationships between terms, such as (“_Scotia_”, “_offers_”, “_savings plan_”) in the first example shown in Figure [2](https://arxiv.org/html/2410.21067v1#S3.F2 "Figure 2 ‣ 3.1 Task Formulation ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models"). By organizing these relationships into nodes and edges within a graph, TransKG provides a structured representation that captures the context-specific details necessary for translation.

#### External Documents Retrieval

We also enable the agent to access external sources, such as online databases or other external corpora. It retrieves domain-specific and bilingual information, such as services provided by “_Scotiabank_” or geographical details of the “_Scotia Sea_”, which are crucial for disambiguating terms with multiple meanings or unfamiliar references.

#### Contextual Knowledge Integration

After validation, the agent could integrate that relevant and appropriate information into TransKG by linking it to the relevant nodes, thereby enriching the contextual understanding — e.g., associating (“_Scotia_”, “_is a_”, “_bank_”) as shown in Figure [2](https://arxiv.org/html/2410.21067v1#S3.F2 "Figure 2 ‣ 3.1 Task Formulation ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models").

This unified graph offers a comprehensive view of the relationships and meanings of terms, enabling us to accurately interpret and resolve ambiguities. TransKG’s dynamic capabilities ensure that the knowledge graph remains relevant and updated, incorporating the latest information needed for accurate translation. The validation process will be detailed in Section [3.4](https://arxiv.org/html/2410.21067v1#S3.SS4 "3.4 Causality-enhanced Judge ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models").

### 3.4 Causality-enhanced Judge

Following the construction of TransKG, the Causality-Enhanced Judge agent evaluates whether retrieved information is contextually relevant and whether ambiguous terms are accurately interpreted based on their intended meanings. For example in Figure [2](https://arxiv.org/html/2410.21067v1#S3.F2 "Figure 2 ‣ 3.1 Task Formulation ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models"), terms like “_bank_” and “_Scotia_” can carry multiple possible meanings depending on context. To ensure causal invariance Pearl ([2009](https://arxiv.org/html/2410.21067v1#bib.bib17)); Pearl and Mackenzie ([2018](https://arxiv.org/html/2410.21067v1#bib.bib18)), the agent uses a causality-driven reflection mechanism, testing whether substituting these terms in translation preserves their original semantic integrity.

The process begins after the framework identifies unknown or ambiguous terms and gathers internal and external knowledge about them. The Causality-enhanced Judge evaluates the contextual implications of these terms, examining if alternative interpretations or translations align with the overall narrative or cause a shift in meaning. Causal invariance demands that, if terms like “_Scotia_” are translated to imply either a financial institution or a geographical location, back-translating into the original language should yield the same conceptual alignment. This can be further clarified through counterfactual reasoning Pearl ([2009](https://arxiv.org/html/2410.21067v1#bib.bib17)); Peters et al. ([2017](https://arxiv.org/html/2410.21067v1#bib.bib20)): if the back-translation introduced unintended meanings, this would lead to a misalignment in understanding. In such cases, the judge discards the retrieved information (i.e., assess them as \[INCORRECT\]), ensuring that only semantically stable knowledge is added to TransKG.

In this way, the Causality-enhanced Judge agent ensures that the external knowledge retrieved is relevant and accurate, avoiding models’ reliance on incorrect or outdated information. This agent also enhances the ability to maintain consistency and accuracy in translation, especially when dealing with polysemy, proper nouns, or emerging terms, etc. By leveraging causality, our framework could verify that the target meaning of unknown terms is logically sound within its context, minimizing the risk of errors and misunderstandings.

### 3.5 Retrieval-Augmented Translator

The Translator agent is the final component of the framework, responsible for producing the translated output using the refined and contextually accurate meanings provided by the earlier agents. Once the Causality-enhanced Judge validates and determines the interpretations of ambiguous terms, the Translator references this adjusted information to generate a precise and coherent translation.

The Translator does not rely solely on direct word-for-word translation. Instead, it incorporates contextual knowledge and causality-aware reflection from other agents to ensure the translation preserves both the semantic integrity and the original intent. For example, if the term “_bank_” is identified as a geographical feature instead of a financial institution, the Translator will use the appropriate term in the target language (e.g., {CJK}UTF8gbsn “河岸” rather than “银行” in Chinese) to convey the correct meaning in the given context.

This ensures that the translated output is not only linguistically accurate but also contextually relevant and logically coherent, providing a natural and fluent translation. By combining the verified meanings from the earlier steps, the Translator achieves a high level of consistency, reducing errors commonly associated with polysemous or context-dependent terms.

<table id="S3.T1.2"><tbody><tr id="S3.T1.2.2"><td id="S3.T1.2.2.3" rowspan="2"><span id="S3.T1.2.2.3.1">Model</span></td><td colspan="3" id="S3.T1.1.1.1"><math alttext="\mathrm{En}\Rightarrow\mathrm{Zh}" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">En</mi><mo id="S3.T1.1.1.1.m1.1.1.1" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.1.cmml">⇒</mo><mi id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml">Zh</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><ci id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1">⇒</ci><ci id="S3.T1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.2">En</ci><ci id="S3.T1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3">Zh</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\mathrm{En}\Rightarrow\mathrm{Zh}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">roman_En ⇒ roman_Zh</annotation></semantics></math></td><td colspan="3" id="S3.T1.2.2.2"><math alttext="\mathrm{Zh}\Rightarrow\mathrm{En}" display="inline" id="S3.T1.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.m1.1a"><mrow id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml"><mi id="S3.T1.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.m1.1.1.2.cmml">Zh</mi><mo id="S3.T1.2.2.2.m1.1.1.1" stretchy="false" xref="S3.T1.2.2.2.m1.1.1.1.cmml">⇒</mo><mi id="S3.T1.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.m1.1.1.3.cmml">En</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1"><ci id="S3.T1.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1.1">⇒</ci><ci id="S3.T1.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.m1.1.1.2">Zh</ci><ci id="S3.T1.2.2.2.m1.1.1.3.cmml" xref="S3.T1.2.2.2.m1.1.1.3">En</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\mathrm{Zh}\Rightarrow\mathrm{En}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.m1.1d">roman_Zh ⇒ roman_En</annotation></semantics></math></td></tr><tr id="S3.T1.2.3"><td id="S3.T1.2.3.1">BLEU</td><td id="S3.T1.2.3.2">COMET</td><td id="S3.T1.2.3.3">CONSIS</td><td id="S3.T1.2.3.4">BLEU</td><td id="S3.T1.2.3.5">COMET</td><td id="S3.T1.2.3.6">CONSIS</td></tr><tr id="S3.T1.2.4"><td id="S3.T1.2.4.1">GPT-3.5-turbo</td><td id="S3.T1.2.4.2">27.6</td><td id="S3.T1.2.4.3">82.0</td><td id="S3.T1.2.4.4">82.1</td><td id="S3.T1.2.4.5">41.9</td><td id="S3.T1.2.4.6">82.2</td><td id="S3.T1.2.4.7">81.3</td></tr><tr id="S3.T1.2.5"><td id="S3.T1.2.5.1">+ <span id="S3.T1.2.5.1.1">CRAT</span></td><td id="S3.T1.2.5.2">28.4</td><td id="S3.T1.2.5.3">82.9</td><td id="S3.T1.2.5.4">86.7</td><td id="S3.T1.2.5.5">43.5</td><td id="S3.T1.2.5.6">82.9</td><td id="S3.T1.2.5.7">84.5</td></tr><tr id="S3.T1.2.6"><td id="S3.T1.2.6.1">GPT-4o</td><td id="S3.T1.2.6.2">32.7</td><td id="S3.T1.2.6.3">83.6</td><td id="S3.T1.2.6.4">83.5</td><td id="S3.T1.2.6.5">43.8</td><td id="S3.T1.2.6.6">82.4</td><td id="S3.T1.2.6.7">82.6</td></tr><tr id="S3.T1.2.7"><td id="S3.T1.2.7.1">+ <span id="S3.T1.2.7.1.1">CRAT</span></td><td id="S3.T1.2.7.2">33.5</td><td id="S3.T1.2.7.3">84.5</td><td id="S3.T1.2.7.4">86.7</td><td id="S3.T1.2.7.5">46.0</td><td id="S3.T1.2.7.6">83.6</td><td id="S3.T1.2.7.7">85.2</td></tr><tr id="S3.T1.2.8"><td id="S3.T1.2.8.1">Qwen-7B-Instruct</td><td id="S3.T1.2.8.2">27.6</td><td id="S3.T1.2.8.3">82.5</td><td id="S3.T1.2.8.4">81.2</td><td id="S3.T1.2.8.5">39.7</td><td id="S3.T1.2.8.6">79.8</td><td id="S3.T1.2.8.7">80.1</td></tr><tr id="S3.T1.2.9"><td id="S3.T1.2.9.1">+ <span id="S3.T1.2.9.1.1">CRAT</span></td><td id="S3.T1.2.9.2">29.2</td><td id="S3.T1.2.9.3">83.3</td><td id="S3.T1.2.9.4">83.8</td><td id="S3.T1.2.9.5">41.8</td><td id="S3.T1.2.9.6">81.0</td><td id="S3.T1.2.9.7">83.0</td></tr><tr id="S3.T1.2.10"><td id="S3.T1.2.10.1">Qwen-72B-Instruct</td><td id="S3.T1.2.10.2">29.9</td><td id="S3.T1.2.10.3">83.0</td><td id="S3.T1.2.10.4">81.7</td><td id="S3.T1.2.10.5">47.1</td><td id="S3.T1.2.10.6">82.4</td><td id="S3.T1.2.10.7">80.6</td></tr><tr id="S3.T1.2.11"><td id="S3.T1.2.11.1">+ <span id="S3.T1.2.11.1.1">CRAT</span></td><td id="S3.T1.2.11.2">30.8</td><td id="S3.T1.2.11.3">84.6</td><td id="S3.T1.2.11.4">85.0</td><td id="S3.T1.2.11.5">50.0</td><td id="S3.T1.2.11.6">83.2</td><td id="S3.T1.2.11.7">83.8</td></tr></tbody></table>

Table 1: Translation performance (%) comparison across various LLMs on New York Times reports. The results are reported with and without the proposed CRAT framework for English-to-Chinese (En⇒Zh⇒EnZh\\mathrm{En}\\Rightarrow\\mathrm{Zh}roman_En ⇒ roman_Zh) and Chinese-to-English (En⇒Zh⇒EnZh\\mathrm{En}\\Rightarrow\\mathrm{Zh}roman_En ⇒ roman_Zh) tasks.

## 4 Experiments

### 4.1 Experimental Setup

#### Dataset

We collect news articles from the New York Times Chinese website 111https://cn.nytimes.com/china/, including both Chinese and English versions. We filter the data to only include reports from 2024 to minimize the likelihood that these reports were encountered during the training phase of LLMs. For each language pair, we retain 500 data points.

#### Baselines

We implement each agent using an LLM and evaluate four LLMs in a zero-shot fashion. For limited-access LLMs, we choose two GPT-series models, GPT-3.5-Turbo-0125 and GPT-4o. We get access to these models through the official API provided by OpenAI and retain the hyper-parameters as default. For open-source LLMs, we deploy Qwen-7B and Qwen-72B. The max_new_tokens is set to 2048 and other hyper-parameters remain default.

#### Metrics

Following previous work, we adopt BLEU Papineni et al. ([2002](https://arxiv.org/html/2410.21067v1#bib.bib16)), reference-free COMET Rei et al. ([2022](https://arxiv.org/html/2410.21067v1#bib.bib21))222 Unbabel/wmt22-comet-da, [https://github.com/Unbabel/COMET/](https://github.com/Unbabel/COMET/), and an evaluator implemented using GPT-4o, which we refer to as CONSIS, to measure translation quality. BLEU and COMET provide quantitative assessments of translation performance — BLEU by comparing n-grams between the translated and reference texts, and COMET by leveraging neural networks to predict human judgment. In contrast, the GPT-4o\-based evaluator (i.e., CONSIS) is designed to assess the translation more holistically and offers a deeper semantic analysis. It focuses on whether the translation accurately conveys the original meaning, with particular focus on the accuracy and consistency of the unknown terms.

### 4.2 Main Results

From Table [1](https://arxiv.org/html/2410.21067v1#S3.T1 "Table 1 ‣ 3.5 Retrieval-Augmented Translator ‣ 3 Method ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models"), we observe that:

(1) Across all models, the integration of CRAT consistently increases BLEU scores, demonstrating that CRAT positively impacts the models’ alignment with reference translations.

(2) The enhancements in COMET and CONSIS scores indicate that CRAT effectively enhances translation accuracy and consistency. The greatest performance enhancements are seen in GPT-4o and Qwen-72B-Instruct, suggesting that more advanced LLMs may better leverage CRAT’s framework.

(3) Among the evaluated models, GPT-4o and Qwen-72B-Instruct with CRAT exhibit the highest scores across all metrics, demonstrating that the combination of an advanced model and the CRAT framework yields superior translation outcomes. In contrast, smaller models like GPT-3.5-turbo and Qwen-7B-Instruct show less pronounced improvements, which may be due to their more limited baseline capabilities.

<table id="S4.T2.1"><tbody><tr id="S4.T2.1.1"><td id="S4.T2.1.1.1"><span id="S4.T2.1.1.1.1">Id</span></td><td id="S4.T2.1.1.2"><span id="S4.T2.1.1.2.1">Setting</span></td><td id="S4.T2.1.1.3"><span id="S4.T2.1.1.3.1">BLEU</span></td><td id="S4.T2.1.1.4"><span id="S4.T2.1.1.4.1">COMET</span></td><td id="S4.T2.1.1.5"><span id="S4.T2.1.1.5.1">CONSIS</span></td></tr><tr id="S4.T2.1.2"><td id="S4.T2.1.2.1">1</td><td id="S4.T2.1.2.2">Vanilla</td><td id="S4.T2.1.2.3">29.9</td><td id="S4.T2.1.2.4">83.0</td><td id="S4.T2.1.2.5">81.7</td></tr><tr id="S4.T2.1.3"><td id="S4.T2.1.3.1">2</td><td id="S4.T2.1.3.2"><span id="S4.T2.1.3.2.1"></span><span id="S4.T2.1.3.2.2"><span id="S4.T2.1.3.2.2.1"><span id="S4.T2.1.3.2.2.1.1"><span id="S4.T2.1.3.2.2.1.1.1">1 + (unrefined) TransKG)</span></span></span></span><span id="S4.T2.1.3.2.3"></span></td><td id="S4.T2.1.3.3">30.4</td><td id="S4.T2.1.3.4">83.9</td><td id="S4.T2.1.3.5">83.6</td></tr><tr id="S4.T2.1.4"><td id="S4.T2.1.4.1">3</td><td id="S4.T2.1.4.2"><span id="S4.T2.1.4.2.1"></span><span id="S4.T2.1.4.2.2"><span id="S4.T2.1.4.2.2.1"><span id="S4.T2.1.4.2.2.1.1"><span id="S4.T2.1.4.2.2.1.1.1">2 + Causality-enhanced</span></span> <span id="S4.T2.1.4.2.2.1.2"><span id="S4.T2.1.4.2.2.1.2.1">Judge (<span id="S4.T2.1.4.2.2.1.2.1.1">CRAT</span>)</span></span></span></span><span id="S4.T2.1.4.2.3"></span></td><td id="S4.T2.1.4.3">30.8</td><td id="S4.T2.1.4.4">84.6</td><td id="S4.T2.1.4.5">85.0</td></tr></tbody></table>

Table 2: Ablation study with Qwen-72B-Instruct on New York Times reports (%) across three configurations: (1) Vanilla, (2) Vanilla with (unrefined) TransKG, and (3) Our proposed CRAT.

### 4.3 Ablation Study

From Table [2](https://arxiv.org/html/2410.21067v1#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models"), configuration 1 denotes vanilla Qwen-72B-Instruct, configuration 2 denotes employing the Unknown Terms Detector and the Knowledge Graph Constructor to obtain an unrefined TransKG, and configuration 3 denotes the entire CRAT framework. We observe that:

(1) Adding TransKG (Configuration 2) leads to improvements in all metrics compared to the Vanilla setup, showing that incorporating internal and external knowledge boosts translation quality and enhances consistency.

(2) Furthermore, Configuration 3 (i.e., our proposed CRAT), which integrates the Causality-Enhanced Judge agent, showing the highest performance across all metrics. This demonstrates that the causality-enhanced reflection mechanism significantly improves not only the accuracy but also the contextual understanding and consistency of translations, underlining the effectiveness of the proposed CRAT framework.

<table id="S4.T3.1"><tbody><tr id="S4.T3.1.1"><td id="S4.T3.1.1.1"><span id="S4.T3.1.1.1.1">Model</span></td><td id="S4.T3.1.1.2"><span id="S4.T3.1.1.2.1">Tranlation</span></td></tr><tr id="S4.T3.1.2"><td id="S4.T3.1.2.1"><span id="S4.T3.1.2.1.1">Source Context</span></td><td id="S4.T3.1.2.2"><em id="S4.T3.1.2.2.1"><span id="S4.T3.1.2.2.1.1"></span><span id="S4.T3.1.2.2.1.2"><span id="S4.T3.1.2.2.1.2.1"><span id="S4.T3.1.2.2.1.2.1.1"><span id="S4.T3.1.2.2.1.2.1.1.1">Even sweeter concoctions await inside Lacaph, a classy new coffeehouse</span></span> <span id="S4.T3.1.2.2.1.2.1.2"><span id="S4.T3.1.2.2.1.2.1.2.1">in District 1, just off <span id="S4.T3.1.2.2.1.2.1.2.1.1">Rach Ben Nghe</span>, the slim urban canal that snakes</span></span> <span id="S4.T3.1.2.2.1.2.1.3"><span id="S4.T3.1.2.2.1.2.1.3.1">through the city. Decorated with dark wood paneling and track lighting,</span></span> <span id="S4.T3.1.2.2.1.2.1.4"><span id="S4.T3.1.2.2.1.2.1.4.1">the cafe serves lemonade (80,000 dong) combined with coffee-blossom honey</span></span> <span id="S4.T3.1.2.2.1.2.1.5"><span id="S4.T3.1.2.2.1.2.1.5.1">and a dose of coffee brewed in a traditional Vietnamese <span id="S4.T3.1.2.2.1.2.1.5.1.1">phin</span>.</span></span></span></span><span id="S4.T3.1.2.2.1.3"></span></em></td></tr><tr id="S4.T3.1.3"><td id="S4.T3.1.3.1">GPT-4o</td><td id="S4.T3.1.3.2"><span id="S4.T3.1.3.2.1">{CJK}</span>UTF8gbsn <span id="S4.T3.1.3.2.2"><span id="S4.T3.1.3.2.2.1">更甜美的饮品在 Lacaph 等着您，这是一家位于第一郡的新时尚咖啡馆，毗邻蜿蜒穿城而过的狭长城市运河——<span id="S4.T3.1.3.2.2.1.1">Rach Ben Nghe</span>。咖啡馆装饰着深色木板墙和轨道灯，提供柠檬水（80,000越南盾），与咖啡花蜜和用传统越南<span id="S4.T3.1.3.2.2.1.2">滤杯（phin）</span>冲泡的咖啡组合而成。</span></span></td></tr><tr id="S4.T3.1.4"><td id="S4.T3.1.4.1">GPT-4o + <span id="S4.T3.1.4.1.1">CRAT</span></td><td id="S4.T3.1.4.2"><span id="S4.T3.1.4.2.1">{CJK}</span>UTF8gbsn <span id="S4.T3.1.4.2.2"><span id="S4.T3.1.4.2.2.1">更甜的混合饮品在等着你, 在第一区新开的优雅咖啡馆Lacaph里。这家咖啡馆就在蜿蜒穿过城市的狭长运河<span id="S4.T3.1.4.2.2.1.1">滨义河</span>边。咖啡馆装饰着深色木板墙和轨道灯，供应柠檬水（80,000越南盾），混合了咖啡花蜂蜜和用传统越南<span id="S4.T3.1.4.2.2.1.2">滴漏咖啡壶</span>冲泡的咖啡。</span></span></td></tr><tr id="S4.T3.1.5"><td id="S4.T3.1.5.1"><span id="S4.T3.1.5.1.1">Source Context</span></td><td id="S4.T3.1.5.2"><em id="S4.T3.1.5.2.1"><span id="S4.T3.1.5.2.1.1"></span><span id="S4.T3.1.5.2.1.2"><span id="S4.T3.1.5.2.1.2.1"><span id="S4.T3.1.5.2.1.2.1.1"><span id="S4.T3.1.5.2.1.2.1.1.1">Every year, Apple and Google announce major software updates that</span></span> <span id="S4.T3.1.5.2.1.2.1.2"><span id="S4.T3.1.5.2.1.2.1.2.1">bring new features to our smartphones, like <span id="S4.T3.1.5.2.1.2.1.2.1.1">cosmetic overhauls</span> to the</span></span> <span id="S4.T3.1.5.2.1.2.1.3"><span id="S4.T3.1.5.2.1.2.1.3.1">home screen, stronger privacy protections and fun messaging tools.</span></span> <span id="S4.T3.1.5.2.1.2.1.4"><span id="S4.T3.1.5.2.1.2.1.4.1">This year, the changes will feel more radical because the companies</span></span> <span id="S4.T3.1.5.2.1.2.1.5"><span id="S4.T3.1.5.2.1.2.1.5.1">are focusing on reinventing our phones with artificial intelligence.</span></span></span></span><span id="S4.T3.1.5.2.1.3"></span></em></td></tr><tr id="S4.T3.1.6"><td id="S4.T3.1.6.1">Qwen-72B-Instruct</td><td id="S4.T3.1.6.2"><span id="S4.T3.1.6.2.1">{CJK}</span>UTF8gbsn <span id="S4.T3.1.6.2.2"><span id="S4.T3.1.6.2.2.1">每年，苹果和谷歌都会宣布重大软件更新，为我们的智能手机带来新功能，如对主屏幕的<span id="S4.T3.1.6.2.2.1.1">美容翻新</span>、更强的隐私保护以及有趣的即时通讯工具。而今年的变化会更加激进，因为这两家公司正致力于利用人工智能重塑我们的手机。</span></span></td></tr><tr id="S4.T3.1.7"><td id="S4.T3.1.7.1"><span id="S4.T3.1.7.1.1"></span><span id="S4.T3.1.7.1.2"><span id="S4.T3.1.7.1.2.1"><span id="S4.T3.1.7.1.2.1.1"><span id="S4.T3.1.7.1.2.1.1.1">Qwen-72B-Instruct</span></span> <span id="S4.T3.1.7.1.2.1.2"><span id="S4.T3.1.7.1.2.1.2.1">+ <span id="S4.T3.1.7.1.2.1.2.1.1">CRAT</span></span></span></span></span><span id="S4.T3.1.7.1.3"></span></td><td id="S4.T3.1.7.2"><span id="S4.T3.1.7.2.1">{CJK}</span>UTF8gbsn <span id="S4.T3.1.7.2.2"><span id="S4.T3.1.7.2.2.1">每年，苹果和谷歌都会宣布重大软件更新，为我们的智能手机带来新功能，比如主屏幕的<span id="S4.T3.1.7.2.2.1.1">外观调整</span>、更强的隐私保护和有趣的即时通讯工具。今年，这些变化会让人感觉更加激进，因为这些公司正致力于用人工智能重塑我们的手机。</span></span></td></tr><tr id="S4.T3.1.8"><td id="S4.T3.1.8.1"><span id="S4.T3.1.8.1.1">Source Context</span></td><td id="S4.T3.1.8.2"><em id="S4.T3.1.8.2.1"><span id="S4.T3.1.8.2.1.1"></span><span id="S4.T3.1.8.2.1.2"><span id="S4.T3.1.8.2.1.2.1"><span id="S4.T3.1.8.2.1.2.1.1"><span id="S4.T3.1.8.2.1.2.1.1.1">Typhoon <span id="S4.T3.1.8.2.1.2.1.1.1.1">Gaemi</span> (2024) (T2403, 05W, Carina) – a powerful</span></span> <span id="S4.T3.1.8.2.1.2.1.2"><span id="S4.T3.1.8.2.1.2.1.2.1">typhoon that impacted East China.</span></span> <span id="S4.T3.1.8.2.1.2.1.3"><span id="S4.T3.1.8.2.1.2.1.3.1"><span id="S4.T3.1.8.2.1.2.1.3.1.1">Gaemi</span> also drenched <span id="S4.T3.1.8.2.1.2.1.3.1.2">western Luzon</span> in the Philippines.</span></span></span></span><span id="S4.T3.1.8.2.1.3"></span></em></td></tr><tr id="S4.T3.1.9"><td id="S4.T3.1.9.1">Qwen-72B-Instruct</td><td id="S4.T3.1.9.2"><span id="S4.T3.1.9.2.1">{CJK}</span>UTF8gbsn <span id="S4.T3.1.9.2.2"><span id="S4.T3.1.9.2.2.1">台风<span id="S4.T3.1.9.2.2.1.1">卡米</span> (2024)（T2403，05W， Carina）是一场强烈的台风，影响了中国东部。<span id="S4.T3.1.9.2.2.1.2">盖米</span>也给菲律宾<span id="S4.T3.1.9.2.2.1.3">西吕宋</span>带来了大量降雨。</span></span></td></tr><tr id="S4.T3.1.10"><td id="S4.T3.1.10.1"><span id="S4.T3.1.10.1.1"></span><span id="S4.T3.1.10.1.2"><span id="S4.T3.1.10.1.2.1"><span id="S4.T3.1.10.1.2.1.1"><span id="S4.T3.1.10.1.2.1.1.1">Qwen-72B-Instruct</span></span> <span id="S4.T3.1.10.1.2.1.2"><span id="S4.T3.1.10.1.2.1.2.1">+ <span id="S4.T3.1.10.1.2.1.2.1.1">CRAT</span></span></span></span></span><span id="S4.T3.1.10.1.3"></span></td><td id="S4.T3.1.10.2"><span id="S4.T3.1.10.2.1">{CJK}</span>UTF8gbsn <span id="S4.T3.1.10.2.2"><span id="S4.T3.1.10.2.2.1">台风<span id="S4.T3.1.10.2.2.1.1">格美</span> (2024)（T2403，05W，卡琳娜）是一场强烈的台风，影响了中国东部。<span id="S4.T3.1.10.2.2.1.2">格美</span>也给菲律宾<span id="S4.T3.1.10.2.2.1.3">吕宋岛西部</span>带来了大量降雨。</span></span></td></tr></tbody></table>

Table 3: Case Study of GPT-4o and Qwen-72B-Instruct when dealing with ambiguous terms such as “_bank_” and “_Scotia_”. Our CRAT framework shows improved accuracy in distinguishing different contextual nuances by integrating knowledge sources and causality-aware reflections, ensuring more consistent and precise translations.

### 4.4 Case Study

In Table [3](https://arxiv.org/html/2410.21067v1#S4.T3 "Table 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models"), we conduct a case study of GPT-4o and Qwen-72B-Instruct to further demonstrate the effectiveness of CRAT in improving translation accuracy, particularly in handling ambiguous or context-sensitive terms. Below is a detailed analysis of the results from these cases.

#### (1) Handling of Polysemes

The first case illustrates the ambiguity of a term “_phin_” in a passage related to a coffeehouse in Vietnam. “_Phin_” refers to a traditional Vietnamese coffee brewing tool. As a polysemous term, "phin" can create confusion since its meaning is context-dependent.

- •GPT-4o translates “_phin_” as “滤杯” (filter bowl), which is a literally correct but contextually inappropriate translation. This misinterpretation highlights GPT-4o’s inability to accurately capture the cultural significance of the term, instead of applying a literal but incorrect meaning.
- •In contrast, GPT-4o + CRAT corrects this by translating “_phin_” as “滴滤咖啡壶” (drip coffee pot). This translation not only accurately conveys the coffee brewing method but also preserves cultural significance, highlighting CRAT’s enhanced ability to manage polysemous terms with cultural relevance.

#### (2) Handling of Proper Nouns

“_Rach Ben Nghe_” in the first case refers to a specific canal in Ho Chi Minh City. It can be seen that:

- •GPT-4o retains the original name “_Rach Ben Nghe_” in its translation, which is acceptable in some cases but can leave readers unfamiliar with Vietnamese geography without adequate context. There is no localization or explanation, making the term harder to understand for a Chinese-speaking user.
- •GPT-4o + CRAT localizes the term to "滨义河" by conducting external knowledge retrieval. This enhances the reader’s comprehension by providing a more accessible translation for Chinese users.

#### (3) Handling of Context-Dependent Terms

The second case addresses a more technical context, where terms like “_cosmetic overhauls_” are discussed in the context of software updates by Apple and Google. These terms are highly domain-specific and require an understanding of both technical jargon and consumer technology language to translate accurately.

- •In this case, Qwen-72B-Instruct without CRAT translates “_cosmetic overhauls_” as “_美容翻新_” (beauty renovation), which is a misinterpretation of the technical meaning. The translation focuses on the literal meaning of “_cosmetic_” rather than the metaphorical use of the term in a software context (i.e., user interface improvements).
- •With CRAT, Qwen-72B-Instruct improves its understanding by translating “_cosmetic overhauls_” as “_外观调整_” (appearance adjustments), which better captures the metaphorical use of the term in the software context.

#### (4) Handling of New Terms

The last case focuses on the translation surrounding Typhoon Gaemi, highlighting the challenges LLMs face in maintaining consistent translations, as well as the importance of accurate handling of new terms.

- •Qwen-72B-Instruct first translates the new term “_Gaemi_” incorrectly as “_卡米_”, and later translates it as “_盖米_”, showing inconsistency in handling the same term. Additionally, the model inaccurately renders “_Western Luzon_” as “_西吕宋_”, which is an improper geographical reference.
- •With CRAT, Qwen-72B-Instruct with CRAT effectively handles this by correctly translating “_Gaemi_” as “_格美_” through retrieving and validating external knowledge about the emerging term. This verifies the capabilities of CRAT to deal with emerging terms and maintain their consistency accurately, highlighting its robustness in handling real-world translation scenarios.

## 5 Conclusion

In this paper, we introduce CRAT, a novel multi-agent framework designed to enhance translation quality by addressing the challenges posed by context-sensitive and emerging terms. Traditional LLM-based translation models often struggle with these terms, leading to inconsistencies, especially when dealing with domain-specific vocabulary or newly coined words. Our proposed approach leverages Retrieval-Augmented Generation (RAG) combined with a causality-enhanced self-reflection mechanism, enabling the model to autonomously identify and clarify these unknown terms with greater accuracy. The framework consists of several specialized agents, each playing a crucial role in ensuring precise translations. By detecting unknown terms, constructing knowledge graphs, and verifying the term meanings through causality-enhanced reflection, CRAT mitigates common translation pitfalls and reduces reliance on manual interventions. Our experimental results demonstrate that CRAT significantly improves translation accuracy, particularly for context-dependent terms, while maintaining consistency across different translation scenarios.

## 6 Limitations

While CRAT demonstrates notable improvements in translation accuracy and consistency, it has several limitations that warrant further exploration. First, the reliance on external knowledge sources during the retrieval process can introduce challenges, particularly when external data is incomplete, outdated, or contains conflicting information, which may lead to translation errors or hallucinations. Additionally, the performance of the framework is dependent on the quality and comprehensiveness of the knowledge graphs constructed, which might not always capture the full context or nuances of certain specialized terms.

Another limitation is the computational complexity introduced by the multi-agent framework, particularly in real-time translation scenarios where speed and efficiency are critical. The need for multiple agents to process, retrieve, and validate information can slow down translation times compared to more traditional, end-to-end models. Lastly, while the framework improves accuracy for unknown and context-sensitive terms, its efficacy in handling highly ambiguous or culturally specific terms still requires further investigation, especially in languages with significant variations in dialects or regional usage. Addressing these challenges will be crucial for the broader application of CRAT in practical, large-scale translation tasks.

## References

- Asai et al. (2023) Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. _arXiv preprint arXiv:2310.11511_.
- Cao et al. (2022) Boxi Cao, Hongyu Lin, Xianpei Han, Fangchao Liu, and Le Sun. 2022. Can prompt probe pretrained language models? understanding the invisible risks from a causal view. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5796–5808.
- Chen et al. (2024) Meiqi Chen, Yubo Ma, Kaitao Song, Yixin Cao, Yan Zhang, and Dongsheng Li. 2024. Improving large language models in event relation logical prediction. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 9451–9478.
- Chesterman (2017) Andrew Chesterman. 2017. A causal model for translation studies. In _Intercultural faultlines_, pages 15–27. Routledge.
- Choi et al. (2017) Heeyoul Choi, Kyunghyun Cho, and Yoshua Bengio. 2017. [Context-dependent word representation for neural machine translation](https://doi.org/10.1016/j.csl.2017.01.007). _Computer Speech and Language_, 45:149–160.
- Conia et al. (2024) Simone Conia, Daniel Lee, Min Li, Umar Farooq Minhas, Saloni Potdar, and Matsushita Li. 2024. Towards cross-cultural machine translation with retrieval-augmented generation from multilingual knowledge graphs. _arXiv preprint arXiv:2410.14057_.
- Ding et al. (2024) Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. _arXiv preprint arXiv:2402.10612_.
- Fiederer and O’Brien (2009) Rebecca Fiederer and Sharon O’Brien. 2009. Quality and machine translation: A realistic objective. _Specialised Translation_.
- Gao et al. (2023) Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6465–6488.
- Ghazvininejad et al. (2023) Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer. 2023. Dictionary-based phrase-level prompting of large language models for machine translation. _arXiv preprint arXiv:2302.07856_.
- He et al. (2024) Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2024. Exploring human-like translation strategy with large language models. _Transactions of the Association for Computational Linguistics_, 12:229–246.
- Hendy et al. (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. _arXiv preprint arXiv:2302.09210_.
- Jiao et al. (2023) Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine. _arXiv preprint arXiv:2301.08745_.
- Ni et al. (2022) Jingwei Ni, Zhijing Jin, Markus Freitag, Mrinmaya Sachan, and Bernhard Schölkopf. 2022. Original or translated? a causal analysis of the impact of translationese on machine translation performance. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5303–5320. Association for Computational Linguistics.
- OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774). _Preprint_, arXiv:2303.08774.
- Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. [Bleu: a method for automatic evaluation of machine translation](https://doi.org/10.3115/1073083.1073135). In _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
- Pearl (2009) Judea Pearl. 2009. _Causality_. Cambridge university press.
- Pearl and Mackenzie (2018) Judea Pearl and Dana Mackenzie. 2018. _The book of why: the new science of cause and effect_. Basic books.
- Peng et al. (2023) Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. 2023. Towards making the most of chatgpt for machine translation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 5622–5633.
- Peters et al. (2017) Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. _Elements of causal inference: foundations and learning algorithms_. The MIT Press.
- Rei et al. (2022) Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. [CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task](https://aclanthology.org/2022.wmt-1.60). In _Proceedings of the Seventh Conference on Machine Translation (WMT)_, pages 634–645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.
- Sun et al. (2024) Zhouhao Sun, Li Du, Xiao Ding, Yixuan Ma, Yang Zhao, Kaitao Qiu, Ting Liu, and Bing Qin. 2024. Causal-guided active learning for debiasing large language models. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 14455–14469.
- Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971). _ArXiv preprint_, abs/2302.13971.
- Vaibhav et al. (2019) Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving robustness of machine translation with synthetic noise. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1916–1920.
- Wang et al. (2024a) Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, and Min Zhang. 2024a. Taste: Teaching large language models to translate through self-reflection. _arXiv preprint arXiv:2406.08434_.
- Wang et al. (2024b) Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, and Min Zhang. 2024b. [Delta: An online document-level translation agent based on multi-level memory](https://arxiv.org/abs/2410.08143). _Preprint_, arXiv:2410.08143.
- Yan et al. (2024) Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. _arXiv preprint arXiv:2401.15884_.
- Ye et al. (2021) Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021. Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering. _arXiv preprint arXiv:2109.08678_.
- Zeng et al. (2024) Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. 2024. [Teaching large language models to translate with comparison](https://doi.org/10.1609/aaai.v38i17.29920). _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(17):19488–19496.
- Zhang et al. (2023) Biao Zhang, Barry Haddow, and Alexandra Birch. 2023. Prompting large language model for machine translation: A case study. In _International Conference on Machine Learning_, pages 41092–41110. PMLR.
- Zhang et al. (2024) Bryan Zhang, Taichi Nakatani, and Stephan Walter. 2024. Enhancing e-commerce product title translation with retrieval-augmented generation and large language models. _arXiv preprint arXiv:2409.12880_.
